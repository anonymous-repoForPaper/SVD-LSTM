{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, \\\n",
    "    mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# LSTM Decompostion=FC+LSTMCell\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDDecomposedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, rank_approx):\n",
    "        super(SVDDecomposedLSTM, self).__init__()\n",
    "        self.rank_approx = rank_approx\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 创建分解权重参数\n",
    "        self.lstm1_ih_u = nn.Parameter(torch.Tensor(4*hidden_size, rank_approx))\n",
    "        self.lstm1_ih_s = nn.Parameter(torch.Tensor(rank_approx))\n",
    "        self.lstm1_ih_v = nn.Parameter(torch.Tensor(rank_approx, input_size))\n",
    "\n",
    "        self.lstm1_hh_u = nn.Parameter(torch.Tensor(4*hidden_size, rank_approx))\n",
    "        self.lstm1_hh_s = nn.Parameter(torch.Tensor(rank_approx))\n",
    "        self.lstm1_hh_v = nn.Parameter(torch.Tensor(rank_approx, hidden_size))\n",
    "\n",
    "        self.lstm2_ih_u = nn.Parameter(torch.Tensor(4*hidden_size, rank_approx))\n",
    "        self.lstm2_ih_s = nn.Parameter(torch.Tensor(rank_approx))\n",
    "        self.lstm2_ih_v = nn.Parameter(torch.Tensor(rank_approx, hidden_size))\n",
    "\n",
    "        self.lstm2_hh_u = nn.Parameter(torch.Tensor(4*hidden_size, rank_approx))\n",
    "        self.lstm2_hh_s = nn.Parameter(torch.Tensor(rank_approx))\n",
    "        self.lstm2_hh_v = nn.Parameter(torch.Tensor(rank_approx, hidden_size))\n",
    "\n",
    "        self.fc_u = nn.Parameter(torch.Tensor(output_size, rank_approx))\n",
    "        self.fc_s = nn.Parameter(torch.Tensor(rank_approx))\n",
    "        self.fc_v = nn.Parameter(torch.Tensor(rank_approx, hidden_size))\n",
    "\n",
    "        self.fc_bias = nn.Parameter(torch.zeros(output_size))  # 全连接层偏置\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # 初始化参数\n",
    "        stdv = 1.0 / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float))\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "        if hidden is None:\n",
    "            # 动态初始化隐藏状态\n",
    "            hidden = ((torch.zeros(batch_size, self.hidden_size, device=x.device),\n",
    "                       torch.zeros(batch_size, self.hidden_size, device=x.device)),\n",
    "                      (torch.zeros(batch_size, self.hidden_size, device=x.device),\n",
    "                       torch.zeros(batch_size, self.hidden_size, device=x.device)))\n",
    "\n",
    "        hx1, cx1 = hidden[0]\n",
    "        hx2, cx2 = hidden[1]\n",
    "\n",
    "        # 使用分解后的权重重建原始权重进行计算\n",
    "        w_ih1 = torch.mm(self.lstm1_ih_u, torch.mm(torch.diag(self.lstm1_ih_s), self.lstm1_ih_v))\n",
    "        w_hh1 = torch.mm(self.lstm1_hh_u, torch.mm(torch.diag(self.lstm1_hh_s), self.lstm1_hh_v))\n",
    "        hx1, cx1 = self.custom_lstm_cell(x, (hx1, cx1), w_ih1, w_hh1)\n",
    "\n",
    "        w_ih2 = torch.mm(self.lstm2_ih_u, torch.mm(torch.diag(self.lstm2_ih_s), self.lstm2_ih_v))\n",
    "        w_hh2 = torch.mm(self.lstm2_hh_u, torch.mm(torch.diag(self.lstm2_hh_s), self.lstm2_hh_v))\n",
    "        hx2, cx2 = self.custom_lstm_cell(hx1, (hx2, cx2), w_ih2, w_hh2)\n",
    "\n",
    "        w_fc = torch.mm(self.fc_u, torch.mm(torch.diag(self.fc_s), self.fc_v))\n",
    "        out = torch.addmm(self.fc_bias, hx2, w_fc.t())\n",
    "\n",
    "        return out, ((hx1, cx1), (hx2, cx2))\n",
    "\n",
    "    def custom_lstm_cell(self, input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n",
    "        hx, cx = hidden  # hx 和 cx 都应该是 [batch_size, hidden_size]\n",
    "\n",
    "        # 确保 input 和 hx 是二维的且具有正确的尺寸\n",
    "        if input.dim() == 3:\n",
    "            input = input.view(-1,\n",
    "                               input.size(2))  # 假设 input 的形状为 [batch_size, 1, input_size]，平展为 [batch_size, input_size]\n",
    "        if hx.dim() == 3:\n",
    "            hx = hx.view(-1, hx.size(2))  # 假设 hx 的形状为 [batch_size, 1, hidden_size]，平展为 [batch_size, hidden_size]\n",
    "\n",
    "        # 应用线性变换\n",
    "        gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)  # gates 应该是 [batch_size, 4*hidden_size]\n",
    "\n",
    "        # 分割门控制向量\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        # 激活函数\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "\n",
    "        return hy, cy\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters define\n",
    "input_size = 7\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "rank_approx=35\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "SVDDecomposedLSTM                        90,056\n",
      "=================================================================\n",
      "Total params: 90,056\n",
      "Trainable params: 90,056\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "model = SVDDecomposedLSTM(input_size, hidden_size, output_size, rank_approx).to(device)\n",
    "print(summary(model))\n",
    "torch.save(model,'modelCache/SVD_LSTM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 7])\n",
      "Output shape: torch.Size([1, 1])\n",
      "Output: tensor([[0.0254]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "seq_len = 1  # Sequence length\n",
    "batch_size = 1  # Number of sequences in the batch\n",
    "x = torch.randn(batch_size, seq_len, input_size).to(device)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    output, _ = model(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcw_torch_gpu_h",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
